{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46058a12-5759-4ac1-b48c-2e14f17cb6a1",
   "metadata": {},
   "source": [
    "# Robust Lookup Table: AI Model\n",
    "\n",
    "The goal of this notebook is to explore and experiment with a deep learning approach to create the Robust Lookup Table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7841dab-03b0-4b03-9479-5cccc304e109",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* #### I. Generate fake scenarios & backends\n",
    "* #### II. Design Deep learning model\n",
    "* #### II. Train model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e3dbf-1d1e-4d59-88ec-4169d3974030",
   "metadata": {},
   "source": [
    "## I. Generate fake scenarios & backends\n",
    "\n",
    "### I.1. Generate fake Scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443cc807-dd0b-463a-b89a-48e79f304667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import uuid\n",
    "import hashlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e62dcb5-b21d-4151-9f85-08e446c60e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Fake scenarios\n",
    "\n",
    "# ScenarioGeneratorConfig\n",
    "# - size: the fixed size of the lookup table.\n",
    "# - nBeforeBounds(x, y): nBefore ∈ [x, y].\n",
    "# - nAfterBounds(x, y): nAfter ∈ [x, y].\n",
    "# - variance(x, y): x < min(nBefore,nAfter)/max(nBefore,nAfter); y < max(nBefore,nAfter) - min(nBefore,nAfter)\n",
    "# # - sizeBounds(x, y): lookup table size ∈ [x, y].\n",
    "class ScenarioGeneratorConfig:\n",
    "    size: int\n",
    "    nBeforeBounds: (int, int)\n",
    "    nAfterBounds: (int, int)\n",
    "    variance: (float, int)\n",
    "    # sizeBounds: (int, int)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int,\n",
    "        nBeforeBounds: (int, int),\n",
    "        nAfterBounds: (int, int),\n",
    "        variance: (float, int),\n",
    "        # sizeBounds: (int, int),\n",
    "    ):\n",
    "        if nBeforeBounds[1] > size or nAfterBounds[1] > size:\n",
    "            raise Exception(\"nBeforeBounds and nAfterBounds cannot exceed size\")\n",
    "\n",
    "        self.size = size\n",
    "        self.nBeforeBounds = nBeforeBounds\n",
    "        self.nAfterBounds = nAfterBounds\n",
    "        self.variance = variance\n",
    "        # self.sizeBounds = sizeBounds\n",
    "\n",
    "class Scenario:\n",
    "    nBefore: int\n",
    "    nAfter: int\n",
    "    size: int\n",
    "\n",
    "def validate_scenario(cfg: ScenarioGeneratorConfig, scenario: Scenario) -> bool:\n",
    "    var = cfg.variance[0]\n",
    "    delta = cfg.variance[1]\n",
    "\n",
    "    _min = min([scenario.nBefore, scenario.nAfter])\n",
    "    _max = max([scenario.nBefore, scenario.nAfter])\n",
    "    _var = _min/_max\n",
    "    _delta = _max - _min\n",
    "    _sz = scenario.size\n",
    "\n",
    "    return _var <= var and _delta <= delta and _max <= _sz and _min != _max\n",
    "\n",
    "# creates a new scenario generator.\n",
    "def new_scenario_generator(cfg):\n",
    "    while True:\n",
    "        scenario = Scenario()\n",
    "        scenario.nBefore = random.randint(cfg.nBeforeBounds[0], cfg.nBeforeBounds[1])\n",
    "        scenario.nAfter = random.randint(cfg.nAfterBounds[0], cfg.nAfterBounds[1])\n",
    "        scenario.size = cfg.size\n",
    "        # scenario.size = random.randint(cfg.sizeBounds[0], cfg.sizeBounds[1])\n",
    "\n",
    "        if validate_scenario(cfg, scenario):\n",
    "            yield scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf07810-4c52-4fba-9ca5-a2d7c9385e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nBefore': 11, 'nAfter': 13, 'size': 47}\n",
      "{'nBefore': 10, 'nAfter': 7, 'size': 47}\n",
      "{'nBefore': 13, 'nAfter': 16, 'size': 47}\n"
     ]
    }
   ],
   "source": [
    "nBeforeBounds = (3, 47)\n",
    "nAfterBounds = (1, 47)\n",
    "variance = (1.0, 10)\n",
    "size = 47\n",
    "\n",
    "cfg = ScenarioGeneratorConfig(size, nBeforeBounds, nAfterBounds, variance)\n",
    "sc = new_scenario_generator(cfg)\n",
    "\n",
    "for i in range(3):\n",
    "    s = next(sc)\n",
    "    print(s.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf4bd8-2cc7-453e-b1cc-fab7a8378a3f",
   "metadata": {},
   "source": [
    "### I.2. Generate fake Backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32047df2-0866-4136-9e1f-430147244ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backend:\n",
    "    id: str\n",
    "    h0: int\n",
    "    h1: int\n",
    "    h2: int\n",
    "    h3: int\n",
    "\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        \n",
    "        _h = hashlib.sha256()\n",
    "        _h.update(self.id.bytes_le)\n",
    "        _b = _h.digest()\n",
    "        self.h0 = int.from_bytes(_b[0:8], \"little\")\n",
    "        self.h1 = int.from_bytes(_b[8:16], \"little\")\n",
    "        self.h2 = int.from_bytes(_b[16:24], \"little\")\n",
    "        self.h3 = int.from_bytes(_b[24:32], \"little\")\n",
    "\n",
    "def list_permutation(l: list, size: int) -> list:\n",
    "    _p = np.random.permutation(l)\n",
    "    return _p[0:size]\n",
    "\n",
    "# new_backend_generator takes a list of Scenario and yields a tuple of 2 Backends.\n",
    "# The \"before\" list and the \"after\" list.\n",
    "def new_backend_generator(scenarioGenerator):\n",
    "    while True:\n",
    "        sc = next(scenarioGenerator)\n",
    "        _min = min([sc.nBefore, sc.nAfter])\n",
    "        _max = max([sc.nBefore, sc.nAfter])\n",
    "        l_min = []\n",
    "        l_max = []\n",
    "\n",
    "        # create the l_max backend array.\n",
    "        l_max = [ Backend() for _ in range(_max) ]\n",
    "        # for _ in range(_max):\n",
    "        #     l_max.append(Backend())\n",
    "\n",
    "        # create l_min array by randomly choosing _min elements of l_max.\n",
    "        l_min = list_permutation(l_max, _min)\n",
    "\n",
    "        # sort both arrays.\n",
    "        l_max = sorted(l_max, key=lambda x: str(x.id))\n",
    "        l_min = sorted(l_min, key=lambda x: str(x.id))\n",
    "        \n",
    "        if sc.nBefore < sc.nAfter:\n",
    "            yield (l_min, l_max)\n",
    "        else:\n",
    "            # print([x.__dict__ for x in l_min])\n",
    "            yield (l_max, l_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c41565-e10a-493d-b945-6a186cd70170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Before: len=46 example_value={'id': UUID('081c6938-5cf1-457e-bb1a-53d9c19d4c33'), 'h0': 12177907672886707155, 'h1': 4090156810695666339, 'h2': 10037499313374949002, 'h3': 15345484821649218001}\n",
      "- After: len=41 example_value={'id': UUID('081c6938-5cf1-457e-bb1a-53d9c19d4c33'), 'h0': 12177907672886707155, 'h1': 4090156810695666339, 'h2': 10037499313374949002, 'h3': 15345484821649218001}\n",
      "- Before: len=14 example_value={'id': UUID('0ed6cee3-3ac6-405b-8718-749764132de2'), 'h0': 9265070034081372038, 'h1': 11708355870753080384, 'h2': 5420950698529392131, 'h3': 5135519590266678383}\n",
      "- After: len=12 example_value={'id': UUID('0ed6cee3-3ac6-405b-8718-749764132de2'), 'h0': 9265070034081372038, 'h1': 11708355870753080384, 'h2': 5420950698529392131, 'h3': 5135519590266678383}\n"
     ]
    }
   ],
   "source": [
    "generator = new_backend_generator(new_scenario_generator(cfg))\n",
    "\n",
    "for i in range(2):\n",
    "    t = next(generator)\n",
    "    print(f\"- Before: len={len(t[0])} example_value={t[0][0].__dict__}\")\n",
    "    print(f\"- After: len={len(t[1])} example_value={t[1][0].__dict__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc854e-b33e-4187-bd5e-334954c70238",
   "metadata": {},
   "source": [
    "## II. Design Deep learning model\n",
    "\n",
    "### Definitions\n",
    "\n",
    "- Let `m` equal to the length of the lookup table: `m=len(lookup_table)`.\n",
    "- Let `input` an array of length equal to m: `len(input)=m`. \n",
    "  - Each entry in `input` represents a backend.\n",
    "- Let `h(i)` the hash of the i-th backend in `input`.\n",
    "- Let `input[i]=h(i) % m`.\n",
    "- Let `n` the number of backend actually represented in `input`.\n",
    "  - Because: `nAfter != nBefore` and `max(nAfter,nBefore) <= m`.\n",
    "- Let `output` a matrix of size `m*m`.\n",
    "- Let `output[j]` the j-th row in the `output` matrix.\n",
    "  - The j-th row of the `output` matrix represents the j-th entry of\n",
    "     the lookup table.\n",
    "- Let `o(i,j)` the i-th entry in `output[j]`.\n",
    "  - `o(i,j)` is the probability of the i-th backend being mapped to the\n",
    "     j-th entry of the lookup table.\n",
    "\n",
    "### Input data\n",
    "\n",
    "Problem 1: How to represent data in the inputs that is unmapped? \n",
    "- E.g. if the modulo of a hash is equal to 0, how should we represent entries\n",
    "  that are out of bound.\n",
    "- In other words, if `n=13` and `m=47`, how do we represent the entries with\n",
    "  index in the range of [13:47]?\n",
    "\n",
    "Definitions:\n",
    "- Let `in-bound` entries the name of entries in the range [0:13].\n",
    "- Let `out-of-bound` entries the name of entries in the range [13:47].\n",
    "\n",
    "Solution:\n",
    "- If we normalize `in-bound` entries as real numbers in [0,1], then we can\n",
    "set `out-of-bound` entries to `-1`.\n",
    "- Another solution would be to represent the input as a `m*m` matrix. \n",
    "  - The i-th row representing the i-th backend.\n",
    "  - The j-th entry in i-th row representing the modulo of the hash of the i-th backend\n",
    "  - If the j-th entry of the i-th row is equal to 1, it means \n",
    "  - If all entries of the i-th row are equal to 0, then it means there are no backends \n",
    "    there.\n",
    "\n",
    "Problem 2: what if multiple backend have the same modulo?\n",
    "- This is particularly problematic if 2 subsequent backends resolves to the same modulo\n",
    "  and 1 of the backend becomes down. \n",
    "- In that case, there is not way to identify which backend was dropped from the model's\n",
    "  point of view. \n",
    "- Hence there is a 50% chance to reaffect packets away from a healthy backend.\n",
    "\n",
    "Solution:\n",
    "- Compute multiple hash for each backend. Or split the 256-bit hash into 4 int64 and \n",
    "  compute 4 modulo. The probability of encountering 4 collisions in the same order\n",
    "  would be significantly lower (the actual improvement has not be calculated).\n",
    "\n",
    "### Model training:\n",
    "\n",
    "- Pass the \"before\" training data through the model.\n",
    "- Pass the \"after\" training data through the model.\n",
    "- Compute \"even distribution\" score: to ensure the backends are evenly\n",
    "  distributed in the output.\n",
    "- Compute \"validity\" score: \n",
    "  - to ensure the model does not make inference `out-of-bound`.\n",
    "- Optional: compute a \"confidence score\", by calculating how likely the top inference\n",
    "  is compared to other o(i,j) value in i-th row.\n",
    "- Compute the % of unchanged entries between \"Before\" and \"After\".\n",
    "- Compute the stability score.\n",
    "- Compute loss function from \"validity\", \"even distribution\" and \"stability\" score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8f3622-b56c-46b8-9272-aaf632881753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a42df5a8-3733-4b84-8bea-68e87911b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output:\n",
    "    # -- size m of the lookup table.\n",
    "    size: int\n",
    "    # -- the raw backend input tensor\n",
    "    raw_input: list\n",
    "    # -- the length of the input (also named n)\n",
    "    input_len: int\n",
    "    # -- the raw tensor output. Size=m*m.\n",
    "    raw_output: list\n",
    "    # -- the cleaned list of Union[uuid,None] output. Size=n (n=len(backends)\n",
    "    output: list[str]\n",
    "    \n",
    "    def __init__(self, size, raw_input, raw_output, output):\n",
    "        self.size = size\n",
    "        self.raw_input = raw_input\n",
    "        self.input_len = len(raw_input)\n",
    "        self.raw_output = raw_output\n",
    "        self.output = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "037a9eff-8568-41cc-aa23-7894c306e87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size': 47,\n",
       " 'raw_input': [<__main__.Backend at 0x7f1e1a536ed0>,\n",
       "  <__main__.Backend at 0x7f1e1a539c70>,\n",
       "  <__main__.Backend at 0x7f1e1a537c80>,\n",
       "  <__main__.Backend at 0x7f1e1a535c70>,\n",
       "  <__main__.Backend at 0x7f1e1a535af0>,\n",
       "  <__main__.Backend at 0x7f1e1a537140>,\n",
       "  <__main__.Backend at 0x7f1e1a5390a0>,\n",
       "  <__main__.Backend at 0x7f1e1a538650>,\n",
       "  <__main__.Backend at 0x7f1e1a538d70>,\n",
       "  <__main__.Backend at 0x7f1e1a535b20>,\n",
       "  <__main__.Backend at 0x7f1e1a537080>,\n",
       "  <__main__.Backend at 0x7f1e1a538890>,\n",
       "  <__main__.Backend at 0x7f1e1a538380>,\n",
       "  <__main__.Backend at 0x7f1e1a535940>,\n",
       "  <__main__.Backend at 0x7f1e1a535df0>,\n",
       "  <__main__.Backend at 0x7f1e1a536e70>,\n",
       "  <__main__.Backend at 0x7f1e1a5351c0>,\n",
       "  <__main__.Backend at 0x7f1e1a537d70>,\n",
       "  <__main__.Backend at 0x7f1e1a538230>,\n",
       "  <__main__.Backend at 0x7f1e1a5379b0>,\n",
       "  <__main__.Backend at 0x7f1e1a537b60>,\n",
       "  <__main__.Backend at 0x7f1e1a536c30>,\n",
       "  <__main__.Backend at 0x7f1e1a537fe0>,\n",
       "  <__main__.Backend at 0x7f1e1a5358b0>,\n",
       "  <__main__.Backend at 0x7f1e1a536b10>,\n",
       "  <__main__.Backend at 0x7f1e1a536b70>,\n",
       "  <__main__.Backend at 0x7f1e1a539820>,\n",
       "  <__main__.Backend at 0x7f1e1a537e30>,\n",
       "  <__main__.Backend at 0x7f1e1a538980>,\n",
       "  <__main__.Backend at 0x7f1e1a5388f0>,\n",
       "  <__main__.Backend at 0x7f1e1a537200>,\n",
       "  <__main__.Backend at 0x7f1e1a5362d0>,\n",
       "  <__main__.Backend at 0x7f1e1a539280>,\n",
       "  <__main__.Backend at 0x7f1e1a536c90>,\n",
       "  <__main__.Backend at 0x7f1e1a536090>,\n",
       "  <__main__.Backend at 0x7f1e1a5382f0>,\n",
       "  <__main__.Backend at 0x7f1e1a535b80>,\n",
       "  <__main__.Backend at 0x7f1e1a5350a0>,\n",
       "  <__main__.Backend at 0x7f1e1a5391c0>,\n",
       "  <__main__.Backend at 0x7f1e1a538290>,\n",
       "  <__main__.Backend at 0x7f1e1a535bb0>,\n",
       "  <__main__.Backend at 0x7f1e1a538f20>,\n",
       "  <__main__.Backend at 0x7f1e1a5364b0>],\n",
       " 'input_len': 43,\n",
       " 'raw_output': tensor([[1.6236e-03, 3.7906e-03, 3.5524e-02,  ..., 1.1886e-03, 4.9089e-04,\n",
       "          3.0229e-02],\n",
       "         [4.4424e-03, 3.6641e-03, 6.0624e-04,  ..., 1.5144e-03, 1.9884e-05,\n",
       "          1.2188e-03],\n",
       "         [3.3148e-03, 7.9392e-04, 2.8376e-03,  ..., 1.1046e-03, 9.2048e-04,\n",
       "          2.0456e-02],\n",
       "         ...,\n",
       "         [8.2957e-03, 5.6543e-04, 1.0021e-02,  ..., 2.3921e-05, 1.0344e-03,\n",
       "          2.4793e-02],\n",
       "         [1.2010e-03, 3.7156e-03, 1.7721e-02,  ..., 5.6543e-03, 1.0629e-02,\n",
       "          2.4177e-02],\n",
       "         [3.6443e-05, 1.6973e-01, 1.4830e-03,  ..., 2.2828e-01, 2.0670e-03,\n",
       "          8.5087e-03]], grad_fn=<SoftmaxBackward0>),\n",
       " 'output': ['313e7a0b-0eb0-4ced-8ecc-84abd1892b34',\n",
       "  '2979ab81-a851-43e3-b53c-db2cf02050c7',\n",
       "  '65de52fd-5e7d-4a63-be24-68b2f66b2b6b',\n",
       "  '3ff2ce45-e6da-4f88-9d8d-3b39d3eee2f6',\n",
       "  'e77c7c13-816f-41e5-99d7-323445d5a01a',\n",
       "  '362bcc7f-bf82-46c6-b989-6b80f7c5b923',\n",
       "  'e0c7c4df-33de-4b50-afc4-7578b201f1f0',\n",
       "  '3ff2ce45-e6da-4f88-9d8d-3b39d3eee2f6',\n",
       "  '07769e30-3029-4966-8e2d-54c42c627adf',\n",
       "  'aa2f191a-2431-4877-8298-25ab651d9f1a',\n",
       "  'e77c7c13-816f-41e5-99d7-323445d5a01a',\n",
       "  '6711c8ac-8afa-47ac-912d-0c787be34fb7',\n",
       "  'c6e76fc4-8d28-4ec3-a34c-34444f16aafa',\n",
       "  '362bcc7f-bf82-46c6-b989-6b80f7c5b923',\n",
       "  '814c0f99-16b6-49c6-84ad-09728199accd',\n",
       "  'b664379a-f26d-49d2-8924-c1ffb4fe8c3b',\n",
       "  'a264e6ba-054a-4f6f-9cd8-8ce7540d877f',\n",
       "  '55566e5b-ee9d-4301-b5ad-6e809ca5ae82',\n",
       "  'aa2f191a-2431-4877-8298-25ab651d9f1a',\n",
       "  'e3849d27-b602-4adc-a41c-9cb2f86c0122',\n",
       "  '3a2a63ef-9907-49fc-aee5-49c92808d93d',\n",
       "  '3a2a63ef-9907-49fc-aee5-49c92808d93d',\n",
       "  '6711c8ac-8afa-47ac-912d-0c787be34fb7',\n",
       "  '07769e30-3029-4966-8e2d-54c42c627adf',\n",
       "  'e0c7c4df-33de-4b50-afc4-7578b201f1f0',\n",
       "  '362bcc7f-bf82-46c6-b989-6b80f7c5b923',\n",
       "  'a686d581-8f8b-41ae-93fb-1cc01dc21424',\n",
       "  'dd22c0fa-5efa-4c98-b820-903d8c9448e4',\n",
       "  'e3849d27-b602-4adc-a41c-9cb2f86c0122',\n",
       "  'cb419853-edd2-4c6a-9caa-7b2c14c258ba',\n",
       "  '84fbf8c3-2509-4209-8837-dc87288363e9',\n",
       "  '29d6c0f9-90af-4319-a00c-cf20b5a4405a',\n",
       "  None,\n",
       "  'a0fec4f6-b8dc-44a1-bc57-e2d939ea8154',\n",
       "  'ed712a8a-eef5-4766-8e66-641926fbf68e',\n",
       "  'aa2f191a-2431-4877-8298-25ab651d9f1a',\n",
       "  '84fbf8c3-2509-4209-8837-dc87288363e9',\n",
       "  '8650fa29-a5a4-42b2-bc14-d8a4eb466274',\n",
       "  '397c53fd-9c86-467c-8f44-cd9783ba02d3',\n",
       "  'a0fec4f6-b8dc-44a1-bc57-e2d939ea8154',\n",
       "  'dd22c0fa-5efa-4c98-b820-903d8c9448e4',\n",
       "  None,\n",
       "  'dd22c0fa-5efa-4c98-b820-903d8c9448e4',\n",
       "  '4172a496-f728-47e3-9bba-ad1ef79add8e',\n",
       "  '814c0f99-16b6-49c6-84ad-09728199accd',\n",
       "  'c6e76fc4-8d28-4ec3-a34c-34444f16aafa',\n",
       "  None]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN takes `m` (int) as a paremeter. \n",
    "# `m` is the length of the lookup table.\n",
    "# input dimensions is a tensor of size `m` and dimension 1.\n",
    "# output dimensions are `m*m` matrices.\n",
    "class NN(nn.Module):\n",
    "    size: int\n",
    "\n",
    "    def __init__(self, m: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = m\n",
    "        self.flatten = nn.Flatten(start_dim=0)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(m*4, m),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(m, m),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(m, m*m),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    # The input is a tuple of ([]Backend, []Backend):\n",
    "    #  - input[0] named `__raw_b_in`, is a list of size `b_len`.\n",
    "    #  - input[1] named `__raw_a_in`, is a list of size `a_len`.\n",
    "    # The output is a tuple of ([]Union(str,None), Union(str,None)).\n",
    "    #  - output[0] is a list of size `b_len`(=len(__raw_b_in)).\n",
    "    #  - output[1] is a list of size `a_len`(=len(__raw_a_in)).\n",
    "    def forward(self, x) -> (list, list):\n",
    "        __raw_b_in, __raw_a_in = x\n",
    "\n",
    "        # -- prepare\n",
    "        __b_in = self.__clean_input(__raw_b_in)\n",
    "        __a_in = self.__clean_input(__raw_a_in)\n",
    "\n",
    "        # -- infer\n",
    "        __b_out = self.__forward_once(__b_in)\n",
    "        __a_out = self.__forward_once(__a_in)\n",
    "\n",
    "        # -- post-process\n",
    "        cleaned_b_out = self.__clean_output(__raw_b_in, __b_out)\n",
    "        cleaned_a_out = self.__clean_output(__raw_a_in, __a_out)\n",
    "\n",
    "        b_out = Output(size, __raw_b_in, __b_out, cleaned_b_out)\n",
    "        a_out = Output(size, __raw_a_in, __a_out, cleaned_a_out)\n",
    "\n",
    "        return b_out, a_out\n",
    "\n",
    "    def __forward_once(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = torch.reshape(x, [self.size, self.size]) # make it a 2-dimensional array.\n",
    "        return self.softmax(x)\n",
    "\n",
    "    def __clean_input(self, x):\n",
    "        out = np.zeros((size,4))\n",
    "        for i, backend in enumerate(x):\n",
    "            # backend[1] is the hash.\n",
    "            out[i][0] = backend.h0 % self.size\n",
    "            out[i][1] = backend.h1 % self.size\n",
    "            out[i][2] = backend.h2 % self.size\n",
    "            out[i][3] = backend.h3 % self.size\n",
    "        return torch.tensor(out, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "    def __clean_output(self, x_in, x_out):\n",
    "        # We want to return a list of length `m` that\n",
    "        # associate each i-th entry with a backend uuid.\n",
    "        # \n",
    "        # An entry at index `i` is obtained by fetching the\n",
    "        # uuid of the backend at index `j` of `x_in`.\n",
    "        # `j` is the index of the highest value of the j-th\n",
    "        # row of `x_out`.\n",
    "        # \n",
    "        # - x_in is the raw input.\n",
    "        # - x_out are matrices of size m*m.\n",
    "        out = []\n",
    "        for row in x_out:\n",
    "            # the first element is the max value.\n",
    "            # we may want to output it in order to calculate the loss.\n",
    "            # this would measure the confidence of the algorithm in the inference. \n",
    "            _, j = torch.max(row, 0) \n",
    "            if j < len(x_in):\n",
    "                out.append(str(x_in[j].id))\n",
    "            else:\n",
    "                out.append(None)\n",
    "        return out\n",
    "\n",
    "model = NN(47).to(device)\n",
    "model(next(generator))[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "76a0b770-0e62-454b-824a-b066ec59d855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9600, 0.2058, 0.4792, 0.6136, 0.0961])\n",
      "tensor([[0],\n",
      "        [3]])\n",
      "n: 2\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5)\n",
    "print(x)\n",
    "aw = torch.argwhere(x > 0.5)\n",
    "print(aw)\n",
    "print(\"n:\", len(aw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f5e8cb98-e897-4657-8f15-99aa8302cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valLoss: tensor(3., requires_grad=True) , distLoss: 0 , stabLoss: 0\n"
     ]
    }
   ],
   "source": [
    "# ValidityLoss is computed as the square of the sum of `out-of-bound` results.\n",
    "class ValidityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValidityLoss, self).__init__()\n",
    "\n",
    "    # raw_output{before,after} are passed as arguments to brain autograd. \n",
    "    def forward(\n",
    "        self, \n",
    "        outputs: (list[Output], list[Output]), \n",
    "        raw_output_before,\n",
    "        raw_output_after,\n",
    "    ):\n",
    "        bef, aft = outputs\n",
    "        \n",
    "        loss = self.__compute_loss(bef.output)\n",
    "        loss += self.__compute_loss(aft.output)\n",
    "\n",
    "        return torch.tensor(float(loss), requires_grad=True)\n",
    "\n",
    "    def __compute_loss(self, x) -> int:\n",
    "        loss = 0\n",
    "        for item in x:\n",
    "            if item is None:\n",
    "               loss +=1\n",
    "        return loss\n",
    "\n",
    "class DistributionLoss(nn.Module):\n",
    "    pass\n",
    "\n",
    "class StabilityLoss(nn.Module):\n",
    "    # raw_output{before,after} are passed as arguments to brain autograd. \n",
    "    def forward(\n",
    "        self, \n",
    "        outputs: (list[Output], list[Output]), \n",
    "        raw_output_before,\n",
    "        raw_output_after,\n",
    "    ):\n",
    "        bef, aft = outputs\n",
    "        \n",
    "        loss = self.__compute_loss(bef.output)\n",
    "        loss += self.__compute_loss(aft.output)\n",
    "\n",
    "        return torch.tensor(float(loss), requires_grad=True)\n",
    "    pass\n",
    "\n",
    "inputs = ([0,1], [2,3])\n",
    "outputs = (\n",
    "    # size, raw_input, raw_output, output\n",
    "    Output(2, [\"noused\",\"notused\"], torch.tensor([[0.,1.],[2.,3.]], requires_grad=True), [\"yolo\", None] ),\n",
    "    Output(3, [\"noused\",\"notused\",\"notused\"], torch.tensor([[0.,1.,2.],[3.,4.,5.]],requires_grad=True), [\"yolo\", None, None] ),\n",
    ")\n",
    "\n",
    "valLoss = ValidityLoss().forward(outputs, None, None)\n",
    "distLoss = 0 # DistributionLoss().forward(outputs)\n",
    "stabLoss = 0 # StabilityLoss().forward(outputs)\n",
    "\n",
    "print(\"valLoss:\", valLoss,\", distLoss:\", distLoss, \", stabLoss:\", stabLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f4ae4-3889-4063-a819-baf7b4b9efb2",
   "metadata": {},
   "source": [
    "## III. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60aed8f5-f55e-45a2-8ff6-97bbd763a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm parameters\n",
    "m = 47\n",
    "nBeforeBounds = (3, m)\n",
    "nAfterBounds = (1, m)\n",
    "variance = (1.0, 10)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-4\n",
    "epochs = 4000\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ea76835-e163-478f-bedc-32eb49a270fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 117171 parameters\n",
      "Parameter name: linear_relu_stack.0.weight, Requires grad: True\n",
      "Parameter name: linear_relu_stack.0.bias, Requires grad: True\n",
      "Parameter name: linear_relu_stack.2.weight, Requires grad: True\n",
      "Parameter name: linear_relu_stack.2.bias, Requires grad: True\n",
      "Parameter name: linear_relu_stack.4.weight, Requires grad: True\n",
      "Parameter name: linear_relu_stack.4.bias, Requires grad: True\n",
      "training model...\n",
      "epoch 1/4000: loss=40.0, elapsed_time=0:00:00.019176\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 101/4000: loss=32.625, elapsed_time=0:00:01.593771\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 201/4000: loss=44.625, elapsed_time=0:00:02.916196\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 301/4000: loss=40.625, elapsed_time=0:00:04.328678\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 401/4000: loss=34.75, elapsed_time=0:00:05.698771\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 501/4000: loss=49.25, elapsed_time=0:00:07.032619\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 601/4000: loss=46.75, elapsed_time=0:00:08.435791\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 701/4000: loss=52.625, elapsed_time=0:00:09.798229\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 801/4000: loss=46.125, elapsed_time=0:00:11.166876\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 901/4000: loss=60.625, elapsed_time=0:00:12.543328\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1001/4000: loss=42.5, elapsed_time=0:00:13.888535\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1101/4000: loss=41.875, elapsed_time=0:00:15.267718\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1201/4000: loss=51.25, elapsed_time=0:00:16.605368\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1301/4000: loss=35.25, elapsed_time=0:00:17.949367\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1401/4000: loss=40.625, elapsed_time=0:00:19.262358\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1501/4000: loss=47.375, elapsed_time=0:00:20.677808\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1601/4000: loss=32.875, elapsed_time=0:00:22.017571\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1701/4000: loss=40.625, elapsed_time=0:00:23.335869\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1801/4000: loss=50.875, elapsed_time=0:00:24.765005\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 1901/4000: loss=38.0, elapsed_time=0:00:26.121574\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2001/4000: loss=35.75, elapsed_time=0:00:27.567411\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2101/4000: loss=46.875, elapsed_time=0:00:28.915829\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2201/4000: loss=39.625, elapsed_time=0:00:30.271358\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2301/4000: loss=44.75, elapsed_time=0:00:31.717180\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2401/4000: loss=38.75, elapsed_time=0:00:33.092367\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2501/4000: loss=41.875, elapsed_time=0:00:34.517530\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2601/4000: loss=43.625, elapsed_time=0:00:35.890029\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2701/4000: loss=53.25, elapsed_time=0:00:37.343697\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2801/4000: loss=49.5, elapsed_time=0:00:38.742515\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 2901/4000: loss=49.75, elapsed_time=0:00:40.111539\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3001/4000: loss=64.375, elapsed_time=0:00:41.552822\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3101/4000: loss=46.125, elapsed_time=0:00:42.936274\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3201/4000: loss=50.875, elapsed_time=0:00:44.359447\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3301/4000: loss=44.0, elapsed_time=0:00:45.756979\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3401/4000: loss=34.25, elapsed_time=0:00:47.145368\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3501/4000: loss=35.375, elapsed_time=0:00:48.568450\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3601/4000: loss=45.5, elapsed_time=0:00:49.954845\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3701/4000: loss=48.75, elapsed_time=0:00:51.372731\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3801/4000: loss=41.625, elapsed_time=0:00:52.803480\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "epoch 3901/4000: loss=53.125, elapsed_time=0:00:54.286332\n",
      "Parameter name: linear_relu_stack.4.bias, data_sample: -0.11141757667064667\n",
      "Training done, elapsed_time=0:00:54.286332\n"
     ]
    }
   ],
   "source": [
    "cfg = ScenarioGeneratorConfig(m, nBeforeBounds, nAfterBounds, variance)\n",
    "backend_generator = new_backend_generator(new_scenario_generator(cfg))\n",
    "\n",
    "model = NN(m).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = ValidityLoss()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"model has {total_params} parameters\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Requires grad: {param.requires_grad}\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"training model...\")\n",
    "for epoch in range(epochs):\n",
    "    # -- reset optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # -- init loss\n",
    "    loss = 0.\n",
    "\n",
    "    # -- run batch\n",
    "    for i in range(batch_size):\n",
    "        # -- generate inputs\n",
    "        inputs = next(backend_generator)\n",
    "        # -- run model\n",
    "        outputs = model(inputs)\n",
    "        # -- compute loss\n",
    "        loss += loss_fn(outputs, outputs[0].raw_output, outputs[1].raw_output)\n",
    "\n",
    "    loss = loss / batch_size\n",
    "    loss.backward()\n",
    "\n",
    "    # -- step\n",
    "    optimizer.step()\n",
    "\n",
    "    # -- log\n",
    "    if epoch % 100 == 0:\n",
    "        elapsed = datetime.timedelta(seconds=(time.time() - start_time))\n",
    "        print(f\"epoch {epoch+1}/{epochs}: loss={loss}, elapsed_time={elapsed}\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if name == \"linear_relu_stack.4.bias\":\n",
    "                print(f\"Parameter name: {name}, data_sample: {param.data[0]}\")\n",
    "\n",
    "print(f\"Training done, elapsed_time={elapsed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ac650-ff3a-4a1d-b56c-c1670808102e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
