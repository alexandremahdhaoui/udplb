{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46058a12-5759-4ac1-b48c-2e14f17cb6a1",
   "metadata": {},
   "source": [
    "# Robust Lookup Table: AI Model\n",
    "\n",
    "The goal of this notebook is to explore and experiment with a deep learning approach to create the Robust Lookup Table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7841dab-03b0-4b03-9479-5cccc304e109",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* #### I. Generate fake scenarios & backends\n",
    "* #### II. Design Deep learning model\n",
    "* #### II. Train model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e3dbf-1d1e-4d59-88ec-4169d3974030",
   "metadata": {},
   "source": [
    "## I. Generate fake scenarios & backends\n",
    "\n",
    "### I.1. Generate fake Scenarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443cc807-dd0b-463a-b89a-48e79f304667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import uuid\n",
    "import hashlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e62dcb5-b21d-4151-9f85-08e446c60e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Fake scenarios\n",
    "\n",
    "# ScenarioGeneratorConfig\n",
    "# - size: the fixed size of the lookup table.\n",
    "# - nBeforeBounds(x, y): nBefore ∈ [x, y].\n",
    "# - nAfterBounds(x, y): nAfter ∈ [x, y].\n",
    "# - variance(x, y): x < min(nBefore,nAfter)/max(nBefore,nAfter); y < max(nBefore,nAfter) - min(nBefore,nAfter)\n",
    "# # - sizeBounds(x, y): lookup table size ∈ [x, y].\n",
    "class ScenarioGeneratorConfig:\n",
    "    size: int\n",
    "    nBeforeBounds: (int, int)\n",
    "    nAfterBounds: (int, int)\n",
    "    variance: (float, int)\n",
    "    # sizeBounds: (int, int)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int,\n",
    "        nBeforeBounds: (int, int),\n",
    "        nAfterBounds: (int, int),\n",
    "        variance: (float, int),\n",
    "        # sizeBounds: (int, int),\n",
    "    ):\n",
    "        if nBeforeBounds[1] > size or nAfterBounds[1] > size:\n",
    "            raise Exception(\"nBeforeBounds and nAfterBounds cannot exceed size\")\n",
    "\n",
    "        self.size = size\n",
    "        self.nBeforeBounds = nBeforeBounds\n",
    "        self.nAfterBounds = nAfterBounds\n",
    "        self.variance = variance\n",
    "        # self.sizeBounds = sizeBounds\n",
    "\n",
    "class Scenario:\n",
    "    nBefore: int\n",
    "    nAfter: int\n",
    "    size: int\n",
    "\n",
    "def validate_scenario(cfg: ScenarioGeneratorConfig, scenario: Scenario) -> bool:\n",
    "    var = cfg.variance[0]\n",
    "    delta = cfg.variance[1]\n",
    "\n",
    "    _min = min([scenario.nBefore, scenario.nAfter])\n",
    "    _max = max([scenario.nBefore, scenario.nAfter])\n",
    "    _var = _min/_max\n",
    "    _delta = _max - _min\n",
    "    _sz = scenario.size\n",
    "\n",
    "    return _var <= var and _delta <= delta and _max <= _sz and _min != _max\n",
    "\n",
    "# creates a new scenario generator.\n",
    "def new_scenario_generator(cfg):\n",
    "    while True:\n",
    "        scenario = Scenario()\n",
    "        scenario.nBefore = random.randint(cfg.nBeforeBounds[0], cfg.nBeforeBounds[1])\n",
    "        scenario.nAfter = random.randint(cfg.nAfterBounds[0], cfg.nAfterBounds[1])\n",
    "        scenario.size = cfg.size\n",
    "        # scenario.size = random.randint(cfg.sizeBounds[0], cfg.sizeBounds[1])\n",
    "\n",
    "        if validate_scenario(cfg, scenario):\n",
    "            yield scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf07810-4c52-4fba-9ca5-a2d7c9385e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nBefore': 13, 'nAfter': 20, 'size': 47}\n",
      "{'nBefore': 8, 'nAfter': 1, 'size': 47}\n",
      "{'nBefore': 39, 'nAfter': 42, 'size': 47}\n"
     ]
    }
   ],
   "source": [
    "nBeforeBounds = (3, 47)\n",
    "nAfterBounds = (1, 47)\n",
    "variance = (1.0, 10)\n",
    "size = 47\n",
    "\n",
    "cfg = ScenarioGeneratorConfig(size, nBeforeBounds, nAfterBounds, variance)\n",
    "sc = new_scenario_generator(cfg)\n",
    "\n",
    "for i in range(3):\n",
    "    s = next(sc)\n",
    "    print(s.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf4bd8-2cc7-453e-b1cc-fab7a8378a3f",
   "metadata": {},
   "source": [
    "### I.2. Generate fake Backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32047df2-0866-4136-9e1f-430147244ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backend:\n",
    "    id: str\n",
    "    h0: int\n",
    "    h1: int\n",
    "    h2: int\n",
    "    h3: int\n",
    "\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        \n",
    "        _h = hashlib.sha256()\n",
    "        _h.update(self.id.bytes_le)\n",
    "        _b = _h.digest()\n",
    "        self.h0 = int.from_bytes(_b[0:8], \"little\")\n",
    "        self.h1 = int.from_bytes(_b[8:16], \"little\")\n",
    "        self.h2 = int.from_bytes(_b[16:24], \"little\")\n",
    "        self.h3 = int.from_bytes(_b[24:32], \"little\")\n",
    "\n",
    "def list_permutation(l: list, size: int) -> list:\n",
    "    _p = np.random.permutation(l)\n",
    "    return _p[0:size]\n",
    "\n",
    "# new_backend_generator takes a list of Scenario and yields a tuple of 2 Backends.\n",
    "# The \"before\" list and the \"after\" list.\n",
    "def new_backend_generator(scenarioGenerator):\n",
    "    while True:\n",
    "        sc = next(scenarioGenerator)\n",
    "        _min = min([sc.nBefore, sc.nAfter])\n",
    "        _max = max([sc.nBefore, sc.nAfter])\n",
    "        l_min = []\n",
    "        l_max = []\n",
    "\n",
    "        # create the l_max backend array.\n",
    "        l_max = [ Backend() for _ in range(_max) ]\n",
    "        # for _ in range(_max):\n",
    "        #     l_max.append(Backend())\n",
    "\n",
    "        # create l_min array by randomly choosing _min elements of l_max.\n",
    "        l_min = list_permutation(l_max, _min)\n",
    "\n",
    "        # sort both arrays.\n",
    "        l_max = sorted(l_max, key=lambda x: str(x.id))\n",
    "        l_min = sorted(l_min, key=lambda x: str(x.id))\n",
    "        \n",
    "        if sc.nBefore < sc.nAfter:\n",
    "            yield (l_min, l_max)\n",
    "        else:\n",
    "            # print([x.__dict__ for x in l_min])\n",
    "            yield (l_max, l_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c41565-e10a-493d-b945-6a186cd70170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Before: len=4 example_value={'id': UUID('3d8d837c-cbe2-4fc4-9d0f-3deed74791b6'), 'h0': 6196111201378782721, 'h1': 17564946255403906249, 'h2': 527157904423247115, 'h3': 8806054692758717150}\n",
      "- After: len=5 example_value={'id': UUID('3d8d837c-cbe2-4fc4-9d0f-3deed74791b6'), 'h0': 6196111201378782721, 'h1': 17564946255403906249, 'h2': 527157904423247115, 'h3': 8806054692758717150}\n",
      "- Before: len=9 example_value={'id': UUID('4c89b4a8-865b-4dd9-b28e-44779fd7728c'), 'h0': 8091776817221854216, 'h1': 7266943767801356441, 'h2': 12464774367246404118, 'h3': 12210405371883127121}\n",
      "- After: len=14 example_value={'id': UUID('0c90ad3d-6b67-4902-be4e-3a45dc934204'), 'h0': 16481765586869143473, 'h1': 14730238227097363266, 'h2': 2905179167379944899, 'h3': 11351632446967968410}\n"
     ]
    }
   ],
   "source": [
    "generator = new_backend_generator(new_scenario_generator(cfg))\n",
    "\n",
    "for i in range(2):\n",
    "    t = next(generator)\n",
    "    print(f\"- Before: len={len(t[0])} example_value={t[0][0].__dict__}\")\n",
    "    print(f\"- After: len={len(t[1])} example_value={t[1][0].__dict__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc854e-b33e-4187-bd5e-334954c70238",
   "metadata": {},
   "source": [
    "## II. Design Deep learning model\n",
    "\n",
    "### Definitions\n",
    "\n",
    "- Let `m` equal to the length of the lookup table: `m=len(lookup_table)`.\n",
    "- Let `input` an array of length equal to m: `len(input)=m`. \n",
    "  - Each entry in `input` represents a backend.\n",
    "- Let `h(i)` the hash of the i-th backend in `input`.\n",
    "- Let `input[i]=h(i) % m`.\n",
    "- Let `n` the number of backend actually represented in `input`.\n",
    "  - Because: `nAfter != nBefore` and `max(nAfter,nBefore) <= m`.\n",
    "- Let `output` a matrix of size `m*m`.\n",
    "- Let `output[j]` the j-th row in the `output` matrix.\n",
    "  - The j-th row of the `output` matrix represents the j-th entry of\n",
    "     the lookup table.\n",
    "- Let `o(i,j)` the i-th entry in `output[j]`.\n",
    "  - `o(i,j)` is the probability of the i-th backend being mapped to the\n",
    "     j-th entry of the lookup table.\n",
    "\n",
    "### Input data\n",
    "\n",
    "Problem 1: How to represent data in the inputs that is unmapped? \n",
    "- E.g. if the modulo of a hash is equal to 0, how should we represent entries\n",
    "  that are out of bound.\n",
    "- In other words, if `n=13` and `m=47`, how do we represent the entries with\n",
    "  index in the range of [13:47]?\n",
    "\n",
    "Definitions:\n",
    "- Let `in-bound` entries the name of entries in the range [0:13].\n",
    "- Let `out-of-bound` entries the name of entries in the range [13:47].\n",
    "\n",
    "Solution:\n",
    "- If we normalize `in-bound` entries as real numbers in [0,1], then we can\n",
    "set `out-of-bound` entries to `-1`.\n",
    "- Another solution would be to represent the input as a `m*m` matrix. \n",
    "  - The i-th row representing the i-th backend.\n",
    "  - The j-th entry in i-th row representing the modulo of the hash of the i-th backend\n",
    "  - If the j-th entry of the i-th row is equal to 1, it means \n",
    "  - If all entries of the i-th row are equal to 0, then it means there are no backends \n",
    "    there.\n",
    "\n",
    "Problem 2: what if multiple backend have the same modulo?\n",
    "- This is particularly problematic if 2 subsequent backends resolves to the same modulo\n",
    "  and 1 of the backend becomes down. \n",
    "- In that case, there is not way to identify which backend was dropped from the model's\n",
    "  point of view. \n",
    "- Hence there is a 50% chance to reaffect packets away from a healthy backend.\n",
    "\n",
    "Solution:\n",
    "- Compute multiple hash for each backend. Or split the 256-bit hash into 4 int64 and \n",
    "  compute 4 modulo. The probability of encountering 4 collisions in the same order\n",
    "  would be significantly lower (the actual improvement has not be calculated).\n",
    "\n",
    "### Model training:\n",
    "\n",
    "- Pass the \"before\" training data through the model.\n",
    "- Pass the \"after\" training data through the model.\n",
    "- Compute \"even distribution\" score: to ensure the backends are evenly\n",
    "  distributed in the output.\n",
    "- Compute \"validity\" score: \n",
    "  - to ensure the model does not make inference `out-of-bound`.\n",
    "- Optional: compute a \"confidence score\", by calculating how likely the top inference\n",
    "  is compared to other o(i,j) value in i-th row.\n",
    "- Compute the % of unchanged entries between \"Before\" and \"After\".\n",
    "- Compute the stability score.\n",
    "- Compute loss function from \"validity\", \"even distribution\" and \"stability\" score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "cf8f3622-b56c-46b8-9272-aaf632881753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a42df5a8-3733-4b84-8bea-68e87911b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output:\n",
    "    # -- n: the length of the input.\n",
    "    n: int\n",
    "    # -- n_max: the max length of n.\n",
    "    n_max: int\n",
    "    # -- m: the size of the lookup table.\n",
    "    m: int\n",
    "    # -- the backend input tensor. (m,4)\n",
    "    input: list\n",
    "    # -- the output tensor. (m, m)\n",
    "    output: list\n",
    "    \n",
    "    def __init__(self, n, n_max, m, input, output):\n",
    "        self.n = n\n",
    "        self.n_max = n_max\n",
    "        self.m = m\n",
    "        self.input = input\n",
    "        self.output = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "037a9eff-8568-41cc-aa23-7894c306e87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n', 'n_max', 'm', 'input', 'output'])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NN takes `m` (int) as a paremeter. \n",
    "# `m` is the length of the lookup table.\n",
    "# input dimensions is a tensor of size `m` and dimension 1.\n",
    "# output dimensions are `m*m` matrices.\n",
    "class NN(nn.Module):\n",
    "    size: int\n",
    "    n_max: int\n",
    "\n",
    "    def __init__(self, m: int, n_max: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = m\n",
    "        self.n_max = n_max\n",
    "        self.flatten = nn.Flatten(start_dim=0)\n",
    "\n",
    "        hidden_layer_size = 512\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(m*4, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_size, m*n_max),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    # The input is a tuple of ([]Backend, []Backend):\n",
    "    #  - input[0] named `__raw_b_in`, is a list of size `b_len`.\n",
    "    #  - input[1] named `__raw_a_in`, is a list of size `a_len`.\n",
    "    # The output is a tuple of ([]Union(str,None), Union(str,None)).\n",
    "    #  - output[0] is a list of size `b_len`(=len(__raw_b_in)).\n",
    "    #  - output[1] is a list of size `a_len`(=len(__raw_a_in)).\n",
    "    def forward(self, x) -> (list, list):\n",
    "        # -- these are list of backends of length n.\n",
    "        b_backends, a_backends = x\n",
    "        b_n, a_n = len(b_backends), len(a_backends)\n",
    "\n",
    "        # -- prepare\n",
    "        b_in = self.__clean_input(b_backends)\n",
    "        a_in = self.__clean_input(a_backends)\n",
    "\n",
    "        # -- run model\n",
    "        b_out = self.__forward_once(b_in)\n",
    "        a_out = self.__forward_once(a_in)\n",
    "\n",
    "        # -- prepare output\n",
    "        bo = Output(b_n, self.n_max, self.size, b_in, b_out) \n",
    "        ao = Output(a_n, self.n_max, self.size, a_in, a_out)\n",
    "        \n",
    "        return bo, ao\n",
    "\n",
    "    def __forward_once(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = nn.functional.normalize(x, dim=0)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        x = torch.reshape(x, [self.size, self.n_max]) # make it a 2-dimensional array.\n",
    "        return self.softmax(x)\n",
    "\n",
    "    def __clean_input(self, x):\n",
    "        out = np.zeros((size,4))\n",
    "        for i, backend in enumerate(x):\n",
    "            # backend[1] is the hash.\n",
    "            out[i][0] = backend.h0 % self.size\n",
    "            out[i][1] = backend.h1 % self.size\n",
    "            out[i][2] = backend.h2 % self.size\n",
    "            out[i][3] = backend.h3 % self.size\n",
    "        return torch.tensor(out, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "# batch_generator = new_batch_generator(\n",
    "#     generator,\n",
    "#     transform_backend_into_tensor_fn,\n",
    "#     32,\n",
    "# )\n",
    "\n",
    "model = NN(47, 13).to(device)\n",
    "model(next(generator))[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "76a0b770-0e62-454b-824a-b066ec59d855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_test_outputs():\n",
    "    b_out = torch.tensor(\n",
    "        [\n",
    "            [0.1, 0.8, 0.1],\n",
    "            [0.1, 0.2, 0.7],\n",
    "            [0.1, 0.2, 0.7],\n",
    "            [0.1, 0.2, 0.7],\n",
    "            [0.2, 0.2, 0.6]\n",
    "        ], \n",
    "        requires_grad=True\n",
    "    )\n",
    "    a_out = torch.tensor(\n",
    "        [\n",
    "            [0.1, 0.8, 0.1],\n",
    "            [0.7, 0.2, 0.1],\n",
    "            [0.7, 0.2, 0.1],\n",
    "            [0.7, 0.2, 0.1],\n",
    "            [0.2, 0.2, 0.6]\n",
    "        ], \n",
    "        requires_grad=True\n",
    "    )\n",
    "    inputs = torch.tensor([[0.,1.], [2.,3.]], requires_grad=True)\n",
    "    return (\n",
    "        # n: int, n_max: int, m: int, input: tensor, output: Output\n",
    "        Output(2, 3, 5, input, b_out),\n",
    "        Output(3, 3, 5, input, a_out),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8577e881-7a9c-48fb-8a3b-6a946695a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valLoss: tensor(5.5000, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ValidityLoss is computed as the square of the sum of `out-of-bound` results.\n",
    "class ValidityLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValidityLoss, self).__init__()\n",
    "\n",
    "    # raw_output{before,after} are passed as arguments to brain autograd. \n",
    "    def forward(\n",
    "        self, \n",
    "        outputs: (list[Output], list[Output]), \n",
    "    ):\n",
    "        bef, aft = outputs\n",
    "\n",
    "        loss = self.__compute_loss(bef)\n",
    "        loss += self.__compute_loss(aft)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # -- Flexible vs Strict approach:\n",
    "    #\n",
    "    # Example of x: n=2, m=3.\n",
    "    #   [0.1, 0.8, 0.1],\n",
    "    #   [0.1, 0.2, 0.7],\n",
    "    #   [0.2, 0.2, 0.6]\n",
    "    #\n",
    "    # FLEXIBLE\n",
    "    #\n",
    "    # In flexible approach we set the largest value of each row of the\n",
    "    # x tensor to 1 and all other values to 0.\n",
    "    # The validity loss is only incremented if the most probable output\n",
    "    # is set \"out-of-bound\".\n",
    "    # This approach is forgiving and allows the model to output non-zero\n",
    "    # out-of-bound.\n",
    "    # Example score would be equal to 2.\n",
    "    # \n",
    "    # STRICT\n",
    "    #\n",
    "    # In strict mode, any value out-of-bound will be accounted for the \n",
    "    # calulation of the loss.\n",
    "    # However, this approach does not account for the criticality of the\n",
    "    # out-of-bound value. Indeed, in the example the out-of-bound value\n",
    "    # of the first row (0.1) does not harm the model at all while the \n",
    "    # out-of-bound value on the second row would be particularly bad.\n",
    "    # Example score would be equal to 3.\n",
    "    # \n",
    "    # CONCLUSION\n",
    "    # \n",
    "    # We can compute and sum both scores.\n",
    "    def __compute_loss(self, out: Output):\n",
    "        n = out.n\n",
    "        n_max = out.n_max\n",
    "        m = out.m\n",
    "        x = out.output\n",
    "        \n",
    "        rev_mask = torch.arange(n_max, dtype=torch.float32) # , requires_grad=True)\n",
    "        rev_mask = torch.where(rev_mask < n, 0., 1.)\n",
    "\n",
    "        xmax = torch.max(x, 1)[0]\n",
    "        xmax = torch.reshape(xmax, (m, 1))\n",
    "        xmax = torch.where(x == xmax, 1., 0.)\n",
    "        xmax = x * xmax\n",
    "        \n",
    "        xoob = torch.where(rev_mask * x > 0, 1., 0.)\n",
    "        xoob = x * xoob\n",
    "\n",
    "        flex_score = torch.sum(rev_mask * xmax)\n",
    "        strict_score = torch.sum(xoob)\n",
    "\n",
    "        return flex_score + strict_score\n",
    "\n",
    "valLoss = ValidityLoss().forward(new_test_outputs())\n",
    "print(\"valLoss:\", valLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "df0e97b9-a9b9-4dc4-9994-43c0141dc7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:\n",
      " tensor([0., 1., 0.], grad_fn=<DivBackward0>)\n",
      "expected:\n",
      " tensor([0.5000, 0.5000, 0.0000])\n",
      "before_loss:\n",
      " tensor(0.3333, grad_fn=<MeanBackward0>)\n",
      "actual:\n",
      " tensor([0.6015, 0.2203, 0.1782], grad_fn=<DivBackward0>)\n",
      "expected:\n",
      " tensor([0.3333, 0.3333, 0.3333])\n",
      "distLoss: tensor(0.5121, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class DistributionLoss(nn.Module):\n",
    "    l1loss: nn.L1Loss\n",
    "    sigmoid: nn.Sigmoid\n",
    "    debug: bool\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DistributionLoss, self).__init__()\n",
    "        self.loss_fn = nn.L1Loss(reduction=\"mean\")\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.debug=False\n",
    "\n",
    "    # raw_output{before,after} are passed as arguments to brain autograd. \n",
    "    def forward(\n",
    "        self, \n",
    "        outputs: (list[Output], list[Output]), \n",
    "    ):\n",
    "        bef, aft = outputs\n",
    "\n",
    "        loss = self.__compute_loss(bef)\n",
    "        if self.debug:\n",
    "            print(\"before_loss:\\n\", loss)\n",
    "        loss += self.__compute_loss(aft)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # Example of x: n=2, m=3.\n",
    "    #   [0.1, 0.8, 0.1],\n",
    "    #   [0.1, 0.2, 0.7],\n",
    "    #   [0.2, 0.2, 0.6]\n",
    "    # => xmax = max(x) * x / x (allow differentiability)\n",
    "    #   [0, 1, 0],\n",
    "    #   [0, 0, 1],\n",
    "    #   [0, 0, 1]\n",
    "    # => masked = xmax * mask\n",
    "    #   [0, 1, 0],\n",
    "    #   [0, 0, 0],\n",
    "    #   [0, 0, 0]\n",
    "    # => sum = sum(masked, 1)\n",
    "    #   [0, 1, 2]\n",
    "    # => actual = sum / m\n",
    "    #   [ 0, 0.33, 0 ]\n",
    "    # => expected = torch.ones(n)/n\n",
    "    #   [ 0.5, 0.5 ]\n",
    "    # => expected = torch.reshape(expected, (m, 1))\n",
    "    #   [ 0.5, 0.5, 0 ]\n",
    "    # Calculate some loss from this\n",
    "    def __compute_loss(self, out: Output):\n",
    "        n = out.n\n",
    "        n_max = out.n_max\n",
    "        m = out.m\n",
    "        x = out.output\n",
    "        \n",
    "        mask = torch.arange(n_max, dtype=torch.float32)\n",
    "        mask = torch.where(mask < n, 1, 0)\n",
    "\n",
    "        xmax = torch.max(x, 1)[0]\n",
    "        xmax = torch.reshape(xmax, (m, 1))\n",
    "        xmax = torch.where(x == xmax, 1, 0)\n",
    "        xmax = xmax * mask\n",
    "        # xmax masked the non maxed x values.\n",
    "\n",
    "        # mask values that aren't relevent\n",
    "        masked = x * xmax # * 10\n",
    "        masked = self.tanh(masked)\n",
    "        # makes the sum of all values.\n",
    "        sum = torch.sum(masked, 0)\n",
    "        total = torch.sum(masked)\n",
    "\n",
    "        actual = sum/total\n",
    "        expected = mask * torch.ones(n_max)/n\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"actual:\\n\",actual)\n",
    "            print(\"expected:\\n\", expected)\n",
    "        \n",
    "        # expected = expected # trying something\n",
    "        # actual = x * actual # trying something\n",
    "        \n",
    "        return self.loss_fn(actual, expected)\n",
    "\n",
    "dist_loss_fn = DistributionLoss()\n",
    "dist_loss_fn.debug = True\n",
    "print(\"distLoss:\", dist_loss_fn.forward(new_test_outputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "6238804d-2b18-44cb-bfc1-c8170383c780",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[5, 1]' is invalid for input of size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m stab_loss_fn \u001b[38;5;241m=\u001b[39m StabilityLoss()\n\u001b[1;32m     52\u001b[0m stab_loss_fn\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mstab_loss_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_test_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[203], line 15\u001b[0m, in \u001b[0;36mStabilityLoss.forward\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     11\u001b[0m     outputs: (\u001b[38;5;28mlist\u001b[39m[Output], \u001b[38;5;28mlist\u001b[39m[Output]), \n\u001b[1;32m     12\u001b[0m ):\n\u001b[1;32m     13\u001b[0m     bef, aft \u001b[38;5;241m=\u001b[39m outputs\n\u001b[0;32m---> 15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore_loss:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "Cell \u001b[0;32mIn[203], line 28\u001b[0m, in \u001b[0;36mStabilityLoss.__compute_loss\u001b[0;34m(self, n, m, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(mask \u001b[38;5;241m<\u001b[39m n, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     27\u001b[0m xmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(x, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m xmax \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m xmax \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(x \u001b[38;5;241m==\u001b[39m xmax, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# xmax masked the non maxed x values.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# mask values that aren't relevent\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[5, 1]' is invalid for input of size 3"
     ]
    }
   ],
   "source": [
    "class StabilityLoss(nn.Module):\n",
    "    debug: bool\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(StabilityLoss, self).__init__()\n",
    "        self.debug=False\n",
    "\n",
    "    # raw_output{before,after} are passed as arguments to brain autograd. \n",
    "    def forward(\n",
    "        self, \n",
    "        outputs: (list[Output], list[Output]), \n",
    "    ):\n",
    "        bef, aft = outputs\n",
    "\n",
    "        loss = self.__compute_loss(bef.n, bef.m, bef.output)\n",
    "        if self.debug:\n",
    "            print(\"before_loss:\\n\", loss)\n",
    "        loss += self.__compute_loss(aft.n, aft.m, aft.output)\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def __compute_loss(self, n: int, m: int, x):\n",
    "        mask = torch.arange(m, dtype=torch.float32)\n",
    "        mask = torch.where(mask < n, 1, 0)\n",
    "\n",
    "        xmax = torch.max(x, 1)[0]\n",
    "        xmax = torch.reshape(xmax, (m, 1))\n",
    "        xmax = torch.where(x == xmax, 1, 0)\n",
    "        # xmax masked the non maxed x values.\n",
    "\n",
    "        # mask values that aren't relevent\n",
    "        masked = (x * xmax) # * mask\n",
    "        masked = nn.Sigmoid()(masked)\n",
    "        # makes the sum of all values.\n",
    "        sum = torch.sum(masked, 0)\n",
    "        total = torch.sum(masked)\n",
    "        \n",
    "        actual = sum/total\n",
    "        expected = mask * torch.ones(m)/n\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"actual:\\n\",actual)\n",
    "            print(\"expected:\\n\", expected)\n",
    "        \n",
    "        # expected = expected # trying something\n",
    "        # actual = x * actual # trying something\n",
    "        \n",
    "        return self.loss_fn(actual, expected)\n",
    "\n",
    "stab_loss_fn = StabilityLoss()\n",
    "stab_loss_fn.debug = True\n",
    "print(\"stabLoss:\", stab_loss_fn.forward(new_test_outputs()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f4ae4-3889-4063-a819-baf7b4b9efb2",
   "metadata": {},
   "source": [
    "## III. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "60aed8f5-f55e-45a2-8ff6-97bbd763a1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm parameters\n",
    "m = 47\n",
    "n_max = int(m/3)\n",
    "nBeforeBounds = (3, n_max)\n",
    "nAfterBounds = (1, n_max)\n",
    "variance = (1.0, 10)\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "epochs = 200\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea76835-e163-478f-bedc-32eb49a270fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 721089 parameters\n",
      "training model...\n",
      "epoch 1/200: loss=0.07440440356731415, elapsed_time=0:00:00.142606\n",
      "epoch 11/200: loss=0.0679997056722641, elapsed_time=0:00:01.476165\n",
      "epoch 21/200: loss=0.07037533074617386, elapsed_time=0:00:02.667176\n",
      "epoch 31/200: loss=0.06389456987380981, elapsed_time=0:00:04.044683\n",
      "epoch 41/200: loss=0.06433159112930298, elapsed_time=0:00:05.270351\n",
      "epoch 51/200: loss=0.06950680911540985, elapsed_time=0:00:06.530006\n",
      "epoch 61/200: loss=0.0670536532998085, elapsed_time=0:00:07.974283\n"
     ]
    }
   ],
   "source": [
    "cfg = ScenarioGeneratorConfig(m, nBeforeBounds, nAfterBounds, variance)\n",
    "backend_generator = new_backend_generator(new_scenario_generator(cfg))\n",
    "# batch_generator = new_batch_generator(\n",
    "#     backend_generator,\n",
    "#     transform_backend_into_tensor_fn,\n",
    "#     batch_size,\n",
    "# )\n",
    "\n",
    "model = NN(m, n_max).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "val_loss_fn = ValidityLoss()\n",
    "dist_loss_fn = DistributionLoss()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"model has {total_params} parameters\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"training model...\")\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    # -- reset optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # -- mean loss\n",
    "    mean_loss = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # -- generate inputs\n",
    "        inputs = next(backend_generator)\n",
    "        # -- run model\n",
    "        outputs = model(inputs)\n",
    "        # -- compute loss\n",
    "        mean_loss += dist_loss_fn(outputs) # + val_loss_fn(outputs)\n",
    "        \n",
    "    # -- propagate loss\n",
    "    mean_loss /= batch_size\n",
    "    mean_loss.backward()\n",
    "    # -- append mean_loss to history & reset mean_loss\n",
    "    loss = mean_loss.detach().numpy()\n",
    "    if loss < 1:\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        \n",
    "    # -- step\n",
    "    optimizer.step()\n",
    "\n",
    "    once = True\n",
    "    if loss < 0.3 and once:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate/10,momentum=0.9)\n",
    "        once = False\n",
    "        \n",
    "    # -- log\n",
    "    # -- display loss of current epoch.\n",
    "    if epoch % 10 != 0 or epoch == epochs - 1:\n",
    "        continue\n",
    "    elapsed = datetime.timedelta(seconds=(time.time() - start_time))\n",
    "    print(f\"epoch {epoch+1}/{epochs}: loss={loss}, elapsed_time={elapsed}\")\n",
    "\n",
    "plt.plot(loss_history, label='loss')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f1cdb-bf85-43eb-8456-0e5fc02fcc74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
